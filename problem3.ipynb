{"cells":[{"cell_type":"markdown","metadata":{"id":"cQ7YzabwjTmk"},"source":["## Εργασία 3 ##\n","\n","Καλωσήρθατε στην τρίτη σας εργασία. Η εργασία αυτή έχει σκοπό να σας βοηθήσει να εμπεδώσετε τα σύνολα μοντέλων.\n","\n","Στην εργασία αυτή θα πρέπει να συμπληρώσετε κώδικα Python 3 στα σημεία που αναφέρουν # YOUR CODE HERE. Μην τροποποιείτε τον κώδικα που βρίσκεται εκτός αυτών των περιοχών.\n","\n","Πρωτού παραδόσετε την εργασία σας σιγουρευτείτε ότι ο κώδικας σε όλα τα κελιά τρέχει σωστά. Για το σκοπό αυτό επιλέξτε από το μενού Χρόνος εκτέλεσης (runtime) -> Επανεκίνηση περιόδου λειτουργίας και εκτέλεση όλων.\n","\n","Συμπληρώστε το όνομα (NAME) και το AEM σας παρακάτω:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"3-rBqXXbjyR0","executionInfo":{"status":"ok","timestamp":1718192935379,"user_tz":-180,"elapsed":5,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["NAME = \"Kyparissis Kyparissis\"\n","AEM = \"10346\""]},{"cell_type":"markdown","metadata":{"id":"egArYhcsTG-T"},"source":["**1** Διαβάστε το διαθέσιμο από την sklearn σύνολο δεδομένων breast cancer, χωρίστε το σε δεδομένα εκπαίδευσης (X_train, y_train) και ελέγχου (X_test, y_test) σε ποσοστό 70%/30% αντίστοιχα με τη συνάρτηση train_test_split (τιμή για random_state βάλτε 0). Το σύνολο αφορά τη διάγνωση καρκίνου του μαστού με βάση μεταβλητές που υπολογίζονται από μια ψηφιοποιημένη εικόνα δείγματος μάζας μαστού που λήφθηκε μέσω αναρρόφησης λεπτής βελόνας (FNA). (2 μονάδες)"]},{"cell_type":"code","execution_count":3,"metadata":{"deletable":false,"id":"qgaPtNAmTCX7","nbgrader":{"cell_type":"code","checksum":"29b99b6039fac516d5fa9c7649e40e1d","grade":false,"grade_id":"cell-407a2dea48bfbf80","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1718192937234,"user_tz":-180,"elapsed":1859,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["# YOUR CODE HERE\n","# --------------\n","\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","\n","# Load the breast cancer dataset\n","breast_cancer = load_breast_cancer()\n","\n","# Split the dataset into training and test sets with a 70% / 30% split\n","test_size = 0.3\n","X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target,\n","                                                    test_size=test_size,\n","                                                    random_state=0)"]},{"cell_type":"code","execution_count":4,"metadata":{"deletable":false,"editable":false,"id":"19LlgZx5cOLP","nbgrader":{"cell_type":"code","checksum":"e4b7add4e469cfc1221bdad01497d404","grade":true,"grade_id":"cell-7387a72f2393ed9a","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1718192937234,"user_tz":-180,"elapsed":5,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["\"\"\"Τεστ ορθής ανάγνωσης και διαχωρισμού του συνόλου δεδομένων\"\"\"\n","assert round(X_train[0][8], 5) == 0.1779\n","assert round(X_test[0][8], 5) == 0.2116"]},{"cell_type":"markdown","metadata":{"id":"RB8RexuPciQr"},"source":["**2** Υλοποιήστε μια ντετερμινιστική εκδοχή της μεθόδου των τυχαίων υποχώρων, η οποία χτίζει τόσα μοντέλα όσες και οι μεταβλητές εισόδου, κάθε ένα από τα οποία αγνοεί και μία διαφορετική μεταβλητή εισόδου. Π.χ. το πρώτο μοντέλο αγνοεί την πρώτη, το δεύτερο αγνοεί τη δεύτερη κτλ. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. (4 μονάδες)"]},{"cell_type":"code","execution_count":5,"metadata":{"deletable":false,"id":"KuC_s04KcigR","nbgrader":{"cell_type":"code","checksum":"5fbe3d9c3d8e46ffc9d9cf06a7644fa3","grade":false,"grade_id":"cell-df57dc0d540a2518","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1718192937235,"user_tz":-180,"elapsed":6,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.base import clone\n","\n","class RandomSubspaceDet:\n","    def __init__(self, estimator=DecisionTreeClassifier()):\n","        # YOUR CODE HERE\n","        # --------------\n","\n","        self.base_estimator = estimator # The base estimator / initial model\n","        self.models = []                # The models trained on the subsets of the data\n","\n","    def fit(self, X_train, y_train):\n","        # YOUR CODE HERE\n","        # --------------\n","\n","        self.models = []                # Reset the models for each train process\n","        n_features = X_train.shape[1]   # Number of features\n","\n","        # Train a model for each subset of the data (i.e. for each feature removed)\n","        # and store the model in the models list\n","        # In loop i, remove the i-th feature from the data\n","        for i in range(n_features):\n","            # Remove the i-th feature from the data and get the new data subset\n","            X_train_subset = np.delete(X_train, i, axis=1)\n","\n","            # Get a clone of the base estimator and train it on the new data subset\n","            model = clone(self.base_estimator)\n","            model.fit(X_train_subset, y_train)\n","\n","            # Store the model in the models list\n","            self.models.append(model)\n","\n","    def predict(self, X):\n","        # YOUR CODE HERE\n","        # --------------\n","\n","        n_samples = X.shape[0]          # Number of samples\n","        n_models = len(self.models)     # Number of models\n","\n","        # Store predictions for each sample of X of each model in a 2D array\n","        predictions = np.zeros((n_samples, n_models))\n","        # Calculate predictions for each model\n","        # (Note again, that the model i needs to be trained on the data without the i-th feature)\n","        for i, model in enumerate(self.models):\n","            # Get again the data subset without the i-th feature\n","            X_subset = np.delete(X, i, axis=1)\n","\n","            # Make predictions for the i-th model using the appropriate data subset (with the i-th feature removed)\n","            predictions[:, i] = model.predict(X_subset)\n","\n","        # Majority voting\n","        # What this does is that for each sample, it counts the number of times each class appears in the predictions\n","        # and returns the class that appears the most\n","        # ex. if for a sample, the predictions are [0, 1, 1, 0, 1], the class 1 appears 3 times and the class 0 appears 2 times\n","        #     -> so the final prediction for this sample will be 1\n","        majority_vote = lambda x: np.bincount(x.astype(int)).argmax()\n","\n","        # Apply the weighted majority vote function to each row of the predictions array\n","        # to get the final predictions of the ensemble for each sample and return them\n","        final_predictions = np.apply_along_axis(majority_vote,\n","                                                axis=1,\n","                                                arr=predictions)\n","        return final_predictions"]},{"cell_type":"code","execution_count":6,"metadata":{"deletable":false,"editable":false,"id":"iDNUeGUEciwi","nbgrader":{"cell_type":"code","checksum":"4db933425279be3712175509b4ba2f53","grade":true,"grade_id":"cell-786f87fa5e67b624","locked":true,"points":4,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1718192937578,"user_tz":-180,"elapsed":348,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["\"\"\"Τεστ ορθής υλοποίησης RandomSubspaceDet\"\"\"\n","from sklearn.metrics import accuracy_score\n","\n","rs = RandomSubspaceDet(estimator=DecisionTreeClassifier(random_state=1))\n","rs.fit(X_train, y_train)\n","assert round(accuracy_score(rs.predict(X_test), y_test), 4) == 0.9006"]},{"cell_type":"markdown","metadata":{"id":"n19eEYRNRnG-"},"source":["**3** Υλοποιήστε τη μέθοδο AdaBoost όπως παρουσιάστηκε στο μάθημα. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. Χρησιμοποιήστε την παράμετρο sample_weight της fit του βασικού μοντέλου για να ορίσετε τα βάρη των παραδειγμάτων εκπαίδευσης. (4 μονάδες)"]},{"cell_type":"code","execution_count":7,"metadata":{"deletable":false,"id":"7NOoKPI8TBX6","nbgrader":{"cell_type":"code","checksum":"7fe3d39eaf3d6a53e2b29f9ebd1e7b6a","grade":false,"grade_id":"cell-946d2440bc05714e","locked":false,"schema_version":3,"solution":true,"task":false},"executionInfo":{"status":"ok","timestamp":1718192937578,"user_tz":-180,"elapsed":3,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.base import clone\n","\n","class AdaBoost:\n","    def __init__(self, n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1)):\n","        # YOUR CODE HERE\n","        # --------------\n","\n","        self.n_estimators = n_estimators    # Number of estimators\n","        self.base_estimator = estimator     # The base estimator / initial model\n","        self.estimators = []                # The trained estimators (models) in the ensemble\n","        self.estimator_weights = []         # The weights of the estimators in the ensemble\n","\n","    def fit(self, X_train, y_train):\n","        # YOUR CODE HERE\n","        # --------------\n","\n","        self.estimators = []                # Reset the estimators for each train process\n","        self.estimator_weights = []         # Reset the estimator weights for each train process\n","        n_samples = X_train.shape[0]        # Number of training samples\n","\n","        # Initialize all the sample weights to w(i) = 1/N, where N is the number of samples\n","        sample_weights = np.ones(n_samples) / n_samples\n","\n","        # Iterate over the number of estimators\n","        for _ in range(self.n_estimators):\n","            # Get a clone of the base estimator and train it using the new sample weights\n","            estimator = clone(self.base_estimator)\n","            estimator.fit(X_train, y_train,\n","                          sample_weight=sample_weights)\n","\n","            # Make predictions using the trained estimator\n","            prediction = estimator.predict(X_train)\n","            # Calculate the total error of the estimator using the sample weights\n","            # The formula is: total_error = sum(w(i) * I(y(i) != h(x(i))))\n","            total_error = 0\n","            for i in range(len(sample_weights)):\n","                if prediction[i] != y_train[i]:\n","                    total_error += sample_weights[i]\n","\n","            # If the total error is 0 or greater than 0.5, break the loop\n","            # If equal to 0, we can't calculate the estimator_weight since we would divide by 0 (precision error)\n","            # If greater than 0.5, the estimator_weight would be negative\n","            if total_error == 0 or total_error > 0.5:\n","                break\n","\n","            # Determine the weight of the estimator in the ensemble for the final prediction\n","            estimator_weight = 0.5 * np.log((1 - total_error)/(total_error))\n","\n","            # Save the estimator and its weight\n","            self.estimators.append(estimator)\n","            self.estimator_weights.append(estimator_weight)\n","\n","            # Update the sample weights using the formula:\n","            # w(i) = w(i) * exp(estimator_weight) if y(i) != h(x(i))\n","            # w(i) = w(i) * exp(-estimator_weight) if y(i) == h(x(i))\n","            for i in range(len(sample_weights)):\n","                if prediction[i] != y_train[i]:\n","                    sample_weights[i] *= np.exp(estimator_weight)\n","                else:\n","                    sample_weights[i] *= np.exp(-estimator_weight)\n","\n","            # Normalize the sample weights so that they sum up to 1 (i.e. w(i) = w(i) / sum(w))\n","            sample_weights = sample_weights / sample_weights.sum()\n","\n","    def predict(self, X):\n","        # YOUR CODE HERE\n","        # --------------\n","\n","        # Get the predictions of each estimator in the ensemble\n","        predictions = []\n","        for estimator in self.estimators:\n","            predictions.append(estimator.predict(X))\n","\n","        # Then stack them in a 2D array\n","        # ex. if we have 3 estimators and 10 samples, the predictions array will have shape (10, 3)\n","        # where each row will contain the predictions of the 3 estimators for the corresponding sample\n","        predictions = np.stack(predictions, axis=1)\n","\n","        # Weighted majority voting\n","        # For each sample, calculate the weighted majority vote of the predictions of the estimators in the ensemble\n","        # The weighted majority vote is calculated as follows:\n","        # For each unique class in the predictions, calculate the sum of the weights of the estimators that predicted this class\n","        # The class with the highest sum of final weights is the final prediction\n","        weighted_majority_vote = lambda x: np.unique(x)[np.argmax([np.where(x==categ, self.estimator_weights, 0).sum() for categ in np.unique(x)])]\n","\n","        # Apply the weighted majority vote function to each row of the predictions array\n","        # to get the final predictions of the ensemble for each sample and return them\n","        final_predictions = np.apply_along_axis(weighted_majority_vote,\n","                                                axis=1,\n","                                                arr=predictions)\n","        return final_predictions"]},{"cell_type":"code","execution_count":8,"metadata":{"deletable":false,"editable":false,"id":"jVjqVcv_tmYk","nbgrader":{"cell_type":"code","checksum":"32aff6a2c5ed06a7b34e982fc295d895","grade":true,"grade_id":"cell-88a2903df26757f7","locked":true,"points":4,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1718192937875,"user_tz":-180,"elapsed":299,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["\"\"\"Τεστ ορθής υλοποίησης AdaBoost\"\"\"\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.metrics import accuracy_score\n","\n","ab = AdaBoost(n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1, random_state=1))\n","ab.fit(X_train, y_train)\n","assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591\n"]},{"cell_type":"code","execution_count":9,"metadata":{"deletable":false,"editable":false,"id":"0QkPMoBNz3T5","nbgrader":{"cell_type":"code","checksum":"71c5a522427b4fc254b65b5d431a767d","grade":false,"grade_id":"cell-4f6f954c3531f480","locked":true,"schema_version":3,"solution":false,"task":false},"executionInfo":{"status":"ok","timestamp":1718192938205,"user_tz":-180,"elapsed":332,"user":{"displayName":"Kyparissis Kyparissis","userId":"18280909670100413703"}}},"outputs":[],"source":["# Ίδιο αποτέλεσμα και με τη κλάση της sklearn\n","ab = AdaBoostClassifier(n_estimators=20, algorithm=\"SAMME\", estimator=DecisionTreeClassifier(max_depth=1, random_state=1))\n","ab.fit(X_train, y_train)\n","assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":0}